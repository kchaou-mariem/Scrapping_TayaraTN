{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7n3Pm6l3BJI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Liste pour stocker toutes les données\n",
        "toutes_donnees = []\n",
        "\n",
        "\"\"\"Scrape toutes les pages jusqu'à ce qu'il n'y ait plus de contenu\"\"\"\n",
        "page = 1\n",
        "has_next_page = True\n",
        "\n",
        "while has_next_page:\n",
        "\n",
        "    print(f\"Traitement de la page {page} du site Tayara ...\")\n",
        "\n",
        "    # URL de la page courante\n",
        "    url_page = f\"https://www.tayara.tn/ads/c/Véhicules/Voitures/?page={page}\"\n",
        "\n",
        "    # Requête pour la page liste\n",
        "    response = requests.get(url_page)\n",
        "    if response.status_code != 200:\n",
        "      print(f\"Erreur HTTP page {page}\")\n",
        "      continue\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Trouver toutes les annonces\n",
        "    annonces = soup.find_all('article')\n",
        "\n",
        "    if not annonces:\n",
        "        print(\"Plus d'annonces trouvées, fin du scraping.\")\n",
        "        has_next_page = False\n",
        "        break\n",
        "\n",
        "    print(f\" {len(annonces)} annonces de voitures trouvées dans les site Tayara\")\n",
        "\n",
        "    # Traiter chaque annonce\n",
        "    for i, annonce in enumerate(annonces):\n",
        "        try:\n",
        "            # Récupérer le lien vers la page détaillée\n",
        "            lien = annonce.find('a')['href']\n",
        "            # annonce.find('a') : Cherche la balise <a> dans l'annonce\n",
        "            # ['href'] : Prend la valeur de l'attribut href\n",
        "            # => Le chemin vers la page de détails de la voiture de l'annonce\n",
        "\n",
        "            url_detail = \"https://www.tayara.tn\" + lien #concaténation pour accéder à la voiture sélectionnée\n",
        "\n",
        "            # Aller sur la page détaillée\n",
        "            response_detail = requests.get(url_detail)\n",
        "            soup_detail = BeautifulSoup(response_detail.content, 'html.parser')\n",
        "            #html.parser comprend le code HTML et le transforme en arbre compréhensible par python\n",
        "\n",
        "            # Extraire le titre et prix\n",
        "            titre = annonce.find('h2')\n",
        "            prix = annonce.find('data') #le prix se trouve dans balise data\n",
        "\n",
        "            # Dictionnaire pour cette annonce avec les informations de base\n",
        "            donnees_annonce = {\n",
        "                'Titre': titre.text.strip() if titre else 'N/A',\n",
        "                'Prix': prix.text.strip() if prix else 'N/A',\n",
        "                'Page': page,\n",
        "                'URL': url_detail\n",
        "            }\n",
        "\n",
        "            # EXTRAIRE LES 11 CARACTÉRISTIQUES DE LA VOITURE\n",
        "            features = soup_detail.find_all('li', class_='col-span-6') #features de voiture sous forme de li avec col-span-6 - on peut trouver 11 au max (10 possible ....)\n",
        "\n",
        "            for feature in features:\n",
        "                label = feature.find('span', class_='text-gray-600/80') #label: titre caractéristique\n",
        "                valeur = feature.find('span', class_='text-gray-700/80') #valeur: valeur caractéristique\n",
        "\n",
        "                if label and valeur:\n",
        "                    donnees_annonce[label.text.strip()] = valeur.text.strip() #supprimer les espaces inutiles\n",
        "\n",
        "            # Ajouter aux données\n",
        "            toutes_donnees.append(donnees_annonce)\n",
        "            print(f\" Annonce {i+1} a {len(features)} caractéristiques\")\n",
        "\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur annonce {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Page {page} terminée\\n\")\n",
        "    # Pause pour éviter de surcharger le site\n",
        "    time.sleep(1)\n",
        "    page += 1  # Passes à la page suivante\n",
        "\n",
        "\n",
        "# Sauvegarder toutes les données en CSV\n",
        "if toutes_donnees:\n",
        "    with open('tayara_scrapping.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "        # Obtenir toutes les colonnes\n",
        "        colonnes = set()\n",
        "\n",
        "\n",
        "        # Parcourir CHAQUE annonce pour trouver TOUTES les colonnes possibles\n",
        "        for annonce in toutes_donnees:\n",
        "            colonnes.update(annonce.keys())\n",
        "\n",
        "        print(\"Colonnes trouvées:\", colonnes)\n",
        "        #pour chaque annonce de voiture on peut trouver diff colonnes selon le nombre de caractéristiques qu'elle a - les colonnes 'Page', 'Titre', 'Prix', 'URL' sont toujours présents\n",
        "\n",
        "        # Ordonner : d'abord les colonnes fixes, puis les caractéristiques\n",
        "        colonnes_fixes = ['Page', 'Titre', 'Prix', 'URL']\n",
        "        colonnes_caracteristiques = [c for c in colonnes if c not in colonnes_fixes]\n",
        "        colonnes_ordonnees = colonnes_fixes + colonnes_caracteristiques\n",
        "\n",
        "        #Crée un objet qui sait insérer des dictionnaires dans un fichier CSV, avec un ordre de colonnes\n",
        "        writer = csv.DictWriter(f, fieldnames=colonnes_ordonnees)\n",
        "        #Écris la première ligne du CSV avec les noms des colonnes\n",
        "        writer.writeheader()\n",
        "        #Écris toutes les lignes de données d'un coup\n",
        "        writer.writerows(toutes_donnees)\n",
        "\n",
        "    print(f\"{len(toutes_donnees)} annonces sauvegardées\")\n",
        "else:\n",
        "    print(\"Aucune donnée trouvée\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
